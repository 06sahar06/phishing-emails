{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7: Fine-Tuning LLM for Phishing Detection\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Open this notebook in Google Colab\n",
    "2. Go to Runtime > Change runtime type > Select GPU (T4)\n",
    "3. Upload your datasets to Colab or mount Google Drive\n",
    "4. Run all cells\n",
    "\n",
    "**Expected Time**: 15-30 minutes with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Datasets\n",
    "\n",
    "Upload these files from your local machine:\n",
    "- `enron_preprocessed_3k.csv`\n",
    "- `combined_preprocessed_2k.csv`\n",
    "\n",
    "Or mount Google Drive if you've uploaded them there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload files directly\n",
    "from google.colab import files\n",
    "print(\"Upload enron_preprocessed_3k.csv:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\nUpload combined_preprocessed_2k.csv:\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Mount Google Drive (uncomment if using Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# enron_path = '/content/drive/MyDrive/phishing-detection/enron_preprocessed_3k.csv'\n",
    "# combined_path = '/content/drive/MyDrive/phishing-detection/combined_preprocessed_2k.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "enron_df = pd.read_csv('enron_preprocessed_3k.csv')\n",
    "combined_df = pd.read_csv('combined_preprocessed_2k.csv')\n",
    "\n",
    "print(f\"Enron: {len(enron_df)} emails\")\n",
    "print(f\"Combined: {len(combined_df)} emails\")\n",
    "\n",
    "# Use Enron for training (80/20 split)\n",
    "train_df, test_enron_df = train_test_split(\n",
    "    enron_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=enron_df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining: {len(train_df)} emails\")\n",
    "print(f\"Test (Enron): {len(test_enron_df)} emails\")\n",
    "print(f\"Test (Combined): {len(combined_df)} emails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Format Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(email_text, label=None):\n",
    "    \"\"\"Format email for instruction tuning\"\"\"\n",
    "    prompt = f\"\"\"Classify this email as 'phishing' or 'legitimate'.\n",
    "\n",
    "Email:\n",
    "{email_text}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    if label is not None:\n",
    "        classification = \"phishing\" if label == 1 else \"legitimate\"\n",
    "        return prompt + f\" {classification}\"\n",
    "    return prompt\n",
    "\n",
    "# Format training data\n",
    "train_texts = [format_prompt(row['text'], row['label']) for _, row in train_df.iterrows()]\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"\\nExample:\\n{train_texts[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# 4-bit quantization for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phishing-finetuned\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=10,\n",
    "    max_steps=200,  # Limit steps for faster training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train Model\n",
    "\n",
    "**This will take 15-30 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take 15-30 minutes with GPU T4\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Training completed in {training_time:.2f} minutes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./phishing-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./phishing-finetuned-final\")\n",
    "\n",
    "print(\"Model saved to ./phishing-finetuned-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_email(email_text, model, tokenizer):\n",
    "    \"\"\"Classify a single email\"\"\"\n",
    "    prompt = format_prompt(email_text)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip().lower()\n",
    "    \n",
    "    # Extract classification\n",
    "    if \"phishing\" in response:\n",
    "        return 1\n",
    "    elif \"legitimate\" in response:\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # Failed to classify\n",
    "\n",
    "def evaluate_dataset(df, model, tokenizer, dataset_name):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    import time\n",
    "    \n",
    "    print(f\"\\nEvaluating on {dataset_name}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    failed = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        pred = classify_email(row['text'], model, tokenizer)\n",
    "        \n",
    "        if pred is not None:\n",
    "            predictions.append(pred)\n",
    "            true_labels.append(row['label'])\n",
    "        else:\n",
    "            failed += 1\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(df)} emails...\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "    speed = len(df) / total_time\n",
    "    success_rate = (len(predictions) / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Results:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:      {accuracy*100:.2f}%\")\n",
    "    print(f\"Precision:     {precision*100:.2f}%\")\n",
    "    print(f\"Recall:        {recall*100:.2f}%\")\n",
    "    print(f\"F1 Score:      {f1*100:.2f}%\")\n",
    "    print(f\"Speed:         {speed:.3f} emails/second\")\n",
    "    print(f\"Success Rate:  {success_rate:.2f}% ({len(predictions)}/{len(df)})\")\n",
    "    print(f\"Failed:        {failed}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'speed': speed,\n",
    "        'success_rate': success_rate,\n",
    "        'failed': failed\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate on Enron Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 emails for faster evaluation\n",
    "test_enron_sample = test_enron_df.sample(n=min(100, len(test_enron_df)), random_state=42)\n",
    "\n",
    "enron_results = evaluate_dataset(test_enron_sample, model, tokenizer, \"Enron Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Evaluate on Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 emails for faster evaluation\n",
    "combined_sample = combined_df.sample(n=min(100, len(combined_df)), random_state=42)\n",
    "\n",
    "combined_results = evaluate_dataset(combined_sample, model, tokenizer, \"Combined Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Compare with Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON WITH OTHER APPROACHES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nEnron Dataset:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Approach':<30} {'Accuracy':<12} {'F1 Score':<12} {'Speed (emails/s)':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Traditional ML':<30} {'98.00%':<12} {'98.03%':<12} {'601,765':<20}\")\n",
    "print(f\"{'Single LLM (Zero-Shot)':<30} {'91.00%':<12} {'90.53%':<12} {'0.625':<20}\")\n",
    "print(f\"{'Fine-Tuned LLM (NEW)':<30} {f'{enron_results[\"accuracy\"]*100:.2f}%':<12} {f'{enron_results[\"f1\"]*100:.2f}%':<12} {f'{enron_results[\"speed\"]:.3f}':<20}\")\n",
    "print(f\"{'Debate System':<30} {'76.00%':<12} {'72.09%':<12} {'0.133':<20}\")\n",
    "print(f\"{'LangGraph':<30} {'55.00%':<12} {'18.18%':<12} {'0.165':<20}\")\n",
    "\n",
    "print(\"\\nCombined Dataset:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Approach':<30} {'Accuracy':<12} {'F1 Score':<12} {'Speed (emails/s)':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Traditional ML':<30} {'99.50%':<12} {'99.50%':<12} {'125,178':<20}\")\n",
    "print(f\"{'Single LLM (Zero-Shot)':<30} {'97.00%':<12} {'96.70%':<12} {'0.453':<20}\")\n",
    "print(f\"{'Fine-Tuned LLM (NEW)':<30} {f'{combined_results[\"accuracy\"]*100:.2f}%':<12} {f'{combined_results[\"f1\"]*100:.2f}%':<12} {f'{combined_results[\"speed\"]:.3f}':<20}\")\n",
    "print(f\"{'Debate System':<30} {'54.00%':<12} {'4.17%':<12} {'0.120':<20}\")\n",
    "print(f\"{'LangGraph':<30} {'53.00%':<12} {'0.00%':<12} {'0.145':<20}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    'enron': enron_results,\n",
    "    'combined': combined_results,\n",
    "    'training_time_minutes': training_time\n",
    "}\n",
    "\n",
    "with open('phase7_finetuned_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to phase7_finetuned_results.json\")\n",
    "\n",
    "# Download results\n",
    "files.download('phase7_finetuned_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Test Individual Emails (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few examples\n",
    "test_emails = [\n",
    "    \"Dear user, your account has been suspended. Click here to verify your identity immediately.\",\n",
    "    \"Hi team, the meeting is scheduled for tomorrow at 2 PM in conference room B.\",\n",
    "    \"URGENT: Your PayPal account will be closed unless you update your information now!\"\n",
    "]\n",
    "\n",
    "print(\"Testing individual emails:\\n\")\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    pred = classify_email(email, model, tokenizer)\n",
    "    classification = \"PHISHING\" if pred == 1 else \"LEGITIMATE\" if pred == 0 else \"FAILED\"\n",
    "    print(f\"Email {i}: {classification}\")\n",
    "    print(f\"Text: {email[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Model fine-tuned successfully  \n",
    "✅ Evaluated on both datasets  \n",
    "✅ Results saved  \n",
    "\n",
    "**Next Steps:**\n",
    "1. Download the results JSON file\n",
    "2. Update your project documentation with Phase 7 results\n",
    "3. Compare fine-tuned performance with other approaches\n",
    "\n",
    "**Expected Improvement:**\n",
    "- Fine-tuned model should perform 2-5% better than zero-shot LLM\n",
    "- Target: 93-97% accuracy (between zero-shot LLM and traditional ML)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
